{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg16_interpretation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CniKr6Fo-3iy",
        "colab_type": "text"
      },
      "source": [
        "## In this notebook, we will load a CNN model called the [VGG16](https://arxiv.org/abs/1409.1556) model, with pre-trained weights on the [ImageNet](http://www.image-net.org) dataset. We will use *activation maximization* to visualize the features that this model has learnt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro0Y6oUL4oC7",
        "colab_type": "text"
      },
      "source": [
        "### ImageNet is an image database organized according to the [WordNet](https://wordnet.princeton.edu/) hierarchy. Each meaningful concept in WordNet is called a \"synonym set\" or \"synset\". ImageNet has more than 100,000 synsets with an average of 1000 images to illustrate each synset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYp7pwZaB-bX",
        "colab_type": "text"
      },
      "source": [
        "### Step 1 Install and import all dependencies. (You can ignore the error messages in the outputs.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78qgQJpFL1kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "!pip install Keras-Applications\n",
        "!pip install --quiet --force-reinstall git+https://github.com/raghakot/keras-vis.git -U\n",
        "!pip install --quiet --force-reinstall scipy==1.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg3twKQyMVTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from vis.visualization import visualize_activation\n",
        "from vis.utils import utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras import activations\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR7wj5UoHnzQ",
        "colab_type": "text"
      },
      "source": [
        "### Step 2 First, let's understand what's inside the ImageNet dataset. Below, we are downloading synonym sets for each of the 1000 classes of images in ImageNet. The descriptions can also be viewed [here.](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ITEW_ADrxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from urllib.request import urlopen\n",
        "classidx_to_description_dict = pickle.load(urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWC5I8XiCCnm",
        "colab_type": "text"
      },
      "source": [
        "### Step 3 Download and load the pre-trained model. For this exercise, we are using the [VGG16](https://arxiv.org/abs/1409.1556) architecture with weights pre-trained on [ImageNet](http://www.image-net.org) dataset. Note that in the last layer (predictions dense layer), the model output is classified into 1000 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt_py1fjuLpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcPATVq_Lh-E",
        "colab_type": "text"
      },
      "source": [
        "### Step 4 Use activation maximization to visualize the images that maximize output of each filter in the prediction layer. Prediction layer is the layer in the model with 1000 filters, each representing a separate class of dataset.\n",
        "### Question: What features has the model learnt for class *Volcano* and *Baseball*, separately? How about other classes? Share with your table the interesting visualizations you get! Modify variable *filter_idx* to explore the features learned by different filters in the prediction layer.</br></br> You can find the *filter name* to *filter index* mapping [here.](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULSSO23rzWIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_layer_name = \"predictions\" # The prediction layer. Refer model.summary() to get names of all layers."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uvG1sHuMPAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_layer_index_from_layer_name(layer_name):\n",
        "  for idx, layer in enumerate(model.layers):\n",
        "    if layer.name == layer_name:\n",
        "        return idx\n",
        "\n",
        "prediction_layer_index = get_layer_index_from_layer_name(prediction_layer_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSPAWCti7Bic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter_idx = 980 # Index of filter you want to visualize. In this case 980 is Volcano. For the predictions layer, this corresponds to class indexes seen in classidx_to_description_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAwUGJJ_QZAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_activation = model.layers[prediction_layer_index].activation\n",
        "\n",
        "# Modify activation of last layer to linear\n",
        "model.layers[prediction_layer_index].activation = activations.linear\n",
        "model = utils.apply_modifications(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-CP_VI2P2Hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig=plt.figure(figsize=(20, 20))\n",
        "print(\"Visualizing image for class index \" + str(filter_idx) + \": \" + classidx_to_description_dict[filter_idx])\n",
        "img = visualize_activation(model, prediction_layer_index, filter_indices=[filter_idx], max_iter=100, tv_weight=1., lp_norm_weight=0.)\n",
        "ax2 = fig.add_subplot(2, 2, 1)\n",
        "plt.imshow(img)\n",
        "\n",
        "# Restoring model to its original state\n",
        "model.layers[prediction_layer_index].activation = original_activation\n",
        "model = utils.apply_modifications(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZl-eBGxmoct",
        "colab_type": "text"
      },
      "source": [
        "### Step 5 Try modifying variable *layer_name* to explore the features learned by different layers.\n",
        "### Question: What features has model learnt for layer block1_conv1, block2_conv1, block3_conv1, block4_conv1 and block5_conv1, separately? Experiment with other layers yourself: From shallow to deeper layers, how are the features learnt evolving?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YzrkSsfa2eq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = \"block1_conv1\" # Name of the layer whose filters you want to visualize. Refer model.summary() to get names of all layers.\n",
        "layer_index = get_layer_index_from_layer_name(layer_name)\n",
        "\n",
        "original_activation = model.layers[layer_index].activation\n",
        "model.layers[layer_index].activation = activations.linear\n",
        "model = utils.apply_modifications(model)\n",
        "\n",
        "columns = 5\n",
        "rows = 2\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for i in range(0,10):\n",
        "  img = visualize_activation(model, layer_index, filter_indices=[i], max_iter=100, tv_weight=1., lp_norm_weight=0.)\n",
        "  fig.add_subplot(rows, columns, i+1)\n",
        "  plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# Restoring model to its original state\n",
        "model.layers[layer_index].activation = original_activation\n",
        "model = utils.apply_modifications(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMhYjtR9u1ET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}